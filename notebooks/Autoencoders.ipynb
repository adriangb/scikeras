{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z22BE9uhvoxO"
   },
   "source": [
    "# Autoencoders in SciKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hapoJed-voxP"
   },
   "source": [
    "Autencoders are an approach to use nearual networks to distill data into it's most important features, thereby compressing the data. We will be following the [Keras tutorial](https://blog.keras.io/building-autoencoders-in-keras.html) on the topic, which goes much more in depth and breadth than we will here. You are highly encouraged to check out that tutorial if you want to learn about autoencoders in the general sense.\n",
    "\n",
    "\n",
    "\n",
    "<table align=\"left\"><td>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/adriangb/scikeras/blob/master/notebooks/Basic_Usage.ipyn\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
    "</td><td>\n",
    "<a target=\"_blank\" href=\"https://github.com/adriangb/scikeras/blob/master/notebooks/Basic_Usage.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT-ibpi7voxQ"
   },
   "source": [
    "### Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekJWKPFMvoxR"
   },
   "source": [
    "* [Data](#Data)\n",
    "* [Define Model](#Define-Keras-Model)\n",
    "* [Training](#Training)\n",
    "* [Explore Results](#Explore-Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6avb3GBQDQyG"
   },
   "source": [
    "Install SciKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCcyTjVkvoxR"
   },
   "outputs": [],
   "source": [
    "!python -m pip install git+https://github.com/adriangb/scikeras.git@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZveNcetDQyL"
   },
   "source": [
    "Silence TensorFlow warnings to keep output succint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ekNmO_GPDQyL"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tensorflow import get_logger\n",
    "get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Sf4j-x4DvoxV"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCuOBH8AvoxX"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3fAUKBUvoxY"
   },
   "source": [
    "We load the dataset from the Keras tutorial. The dataset consists of images of cats and dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "QM74xeoe-1S-",
    "outputId": "23946eec-c32f-4db6-8bda-e12bda6492ea"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 784)\n(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuR10hymK0dh"
   },
   "source": [
    "## Define Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be defining a very simple autencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dBFFXT-__7KU"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder_model = keras.Model(input_img, decoded)\n",
    "\n",
    "# This model maps an input to its encoded representation\n",
    "encoder_model = keras.Model(input_img, encoded)\n",
    "\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder_model.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder_model = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "source": [
    "We create a small class that will allow us to call `fit_transform` instead of having to call `fit` and `transform` sequentially."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from scikeras.wrappers import BaseWrapper\n",
    "\n",
    "\n",
    "class AutoEncoderTransformer(BaseWrapper, TransformerMixin):\n",
    "    \"\"\"A mixin that enables the transform and fit_transform methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we wrap the Keras Model with Scikeras. Note that we use BaseWrapper instead of KerasClassifier or KerasRegressor since this is not a classificaiton or regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_params = {\"optimizer\": \"adam\", \"loss\": \"binary_crossentropy\"}\n",
    "fit_params = {\"epochs\": 50, \"batch_size\": 256, \"shuffle\": True}\n",
    "\n",
    "autoencoder = BaseWrapper(model=autoencoder_model, **compile_params, **fit_params)\n",
    "encoder = AutoEncoderTransformer(model=encoder_model, **compile_params, **fit_params, verbose=0)\n",
    "decoder = AutoEncoderTransformer(model=decoder_model, **compile_params, **fit_params, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqBl5xLDP43O"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fHbt_mUVBhE"
   },
   "source": [
    "To train the model, we pass the input images as both the features and the target. This will train the layers to compress the data as accurately as possible between the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QQhtIWsMP4Ii",
    "outputId": "1581384e-7012-4816-d4cf-a4190a30e50f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0926\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0926\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0926\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0925\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0924\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0924\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0924\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0924\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0924\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0923\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0923\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BaseWrapper(\n",
       "\tmodel=<tensorflow.python.keras.engine.functional.Functional object at 0x147acf670>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adam\n",
       "\tloss=binary_crossentropy\n",
       "\tmetrics=None\n",
       "\tbatch_size=256\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the encoder and decoder using training data and then round-trip through both using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.fit(encoder.fit_transform(x_train))\n",
    "roundtrip_imgs = decoder.transform(encoder.transform(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our inputs to lossy decoded outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(roundtrip_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "What about the compression? Let's check the sizes of the arrays."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_test.shape[1]: 784\nencoded_imgs.shape[1]: 32\nSpace saving: 95.92%\n"
     ]
    }
   ],
   "source": [
    "encoded_imgs = encoder.transform(x_test)\n",
    "space_saving = (1-encoded_imgs.astype(x_test.dtype).nbytes/x_test.nbytes)*100\n",
    "print(f\"x_test.shape[1]: {x_test.shape[1]}\")\n",
    "print(f\"encoded_imgs.shape[1]: {encoded_imgs.shape[1]}\")\n",
    "space_saving = (1-encoded_imgs.astype(x_test.dtype).nbytes/x_test.nbytes)*100\n",
    "print(f\"Space saving: {space_saving:0.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Benchmarks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}