{
 "cells": [
  {
   "cell_type": "raw",
   "id": "decimal-arbitration",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adriangb/scikeras/blob/docs-deploy/refs/heads/master/notebooks/MLPClassifier_MLPRegressor.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Run in Google Colab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-hayes",
   "metadata": {},
   "source": [
    "# MLPClassifier and MLPRegressor in SciKeras\n",
    "\n",
    "SciKeras is a bridge between Keras and Scikit-Learn. As such, one of SciKeras' design goals is to be able to create a Scikit-Learn style estimator backed by Keras.\n",
    "\n",
    "This notebook implements an estimator that is analogous to `sklearn.neural_network.MLPClassifier` using Keras. This estimator should (for the most part) work as a drop-in replacement for `MLPClassifier`!\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* [1. Setup](#1.-Setup)\n",
    "* [2. Defining the Keras Model](#2.-Defining-the-Keras-Model)\n",
    "  * [2.1 Inputs](#2.1-Inputs)\n",
    "  * [2.2 Hidden layers](#2.2-Hidden-layers)\n",
    "  * [2.3 Output layers](#2.3-Output-layers)\n",
    "  * [2.4 Losses and optimizer](#2.4-Losses-and-optimizer)\n",
    "  * [2.5 Wrapping with SciKeras](#2.5-Wrapping-with-SciKeras)\n",
    "* [3. Testing our classifier](#3.-Testing-our-classifier)\n",
    "* [4. Self contained MLPClassifier](#4.-Self-contained-MLPClassifier)\n",
    "  * [4.1 Subclassing](#4.1-Subclassing)\n",
    "* [5. MLPRegressor](#5.-MLPRegressor)\n",
    "\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "practical-people",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:14.806810Z",
     "iopub.status.busy": "2021-01-29T02:15:14.805867Z",
     "iopub.status.idle": "2021-01-29T02:15:17.177311Z",
     "shell.execute_reply": "2021-01-29T02:15:17.176347Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import scikeras\n",
    "except ImportError:\n",
    "    !python -m pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-pepper",
   "metadata": {},
   "source": [
    "Silence TensorFlow logging to keep output succinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sized-answer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.184350Z",
     "iopub.status.busy": "2021-01-29T02:15:17.183127Z",
     "iopub.status.idle": "2021-01-29T02:15:17.185271Z",
     "shell.execute_reply": "2021-01-29T02:15:17.186026Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tensorflow import get_logger\n",
    "get_logger().setLevel('ERROR')\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting the random state for TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatty-colony",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.189852Z",
     "iopub.status.busy": "2021-01-29T02:15:17.188921Z",
     "iopub.status.idle": "2021-01-29T02:15:17.870797Z",
     "shell.execute_reply": "2021-01-29T02:15:17.876789Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-attendance",
   "metadata": {},
   "source": [
    "## 2. Defining the Keras Model\n",
    "\n",
    "First, we outline our model building function, using a `Sequential` Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imposed-drain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.883310Z",
     "iopub.status.busy": "2021-01-29T02:15:17.882613Z",
     "iopub.status.idle": "2021-01-29T02:15:17.886997Z",
     "shell.execute_reply": "2021-01-29T02:15:17.887621Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_model():\n",
    "    model = keras.Sequential()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-equation",
   "metadata": {},
   "source": [
    "### 2.1 Inputs\n",
    "\n",
    "We need to define an input layer for Keras. SciKeras allows you to dynamically determine the input size based on the features (`X`). To do this, you need to add the `meta` parameter to `get_clf_model`'s parameters. `meta` will be a dictionary with all of the `meta` attributes that `KerasClassifier` generates during the `fit` call, including `n_features_in_`, which we will use to dynamically size the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infrared-custom",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.894375Z",
     "iopub.status.busy": "2021-01-29T02:15:17.893542Z",
     "iopub.status.idle": "2021-01-29T02:15:17.897516Z",
     "shell.execute_reply": "2021-01-29T02:15:17.898353Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, Any\n",
    "\n",
    "\n",
    "def get_clf_model(meta: Dict[str, Any]):\n",
    "    model = keras.Sequential()\n",
    "    inp = keras.layers.Input(shape=(meta[\"n_features_in_\"]))\n",
    "    model.add(inp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-freedom",
   "metadata": {},
   "source": [
    "### 2.2 Hidden Layers\n",
    "\n",
    "Multilayer perceptrons are generally composed of an input layer, an output layer and 0 or more hidden layers. The size of the hidden layers is specified via the `hidden_layer_sizes` parameter in MLClassifier, where the the ith element represents the number of neurons in the ith hidden layer. Let's add that parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respected-prototype",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.902416Z",
     "iopub.status.busy": "2021-01-29T02:15:17.901225Z",
     "iopub.status.idle": "2021-01-29T02:15:17.908721Z",
     "shell.execute_reply": "2021-01-29T02:15:17.909728Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_model(hidden_layer_sizes: Iterable[int], meta: Dict[str, Any]):\n",
    "    model = keras.Sequential()\n",
    "    inp = keras.layers.Input(shape=(meta[\"n_features_in_\"]))\n",
    "    model.add(inp)\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        layer = keras.layers.Dense(hidden_layer_size, activation=\"relu\")\n",
    "        model.add(layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-roommate",
   "metadata": {},
   "source": [
    "### 2.3 Output layers\n",
    "\n",
    "The output layer needs to reflect the type of classification task being performed. Here, we will handle 2 cases:\n",
    "\n",
    "- binary classification: single output unit with sigmoid activation\n",
    "- multiclass classification: one output unit for each class, with softmax activation\n",
    "The main complication arises from determining which one to use. Like with the input features, SciKeras provides useful information on the target within the `meta` parameter. Specifically, we will use the `n_classes_` and `target_type_` attributes to determine the number of output units and activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dominican-effectiveness",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.914155Z",
     "iopub.status.busy": "2021-01-29T02:15:17.912777Z",
     "iopub.status.idle": "2021-01-29T02:15:17.921341Z",
     "shell.execute_reply": "2021-01-29T02:15:17.922248Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_model(hidden_layer_sizes: Iterable[int], meta: Dict[str, Any]):\n",
    "    model = keras.Sequential()\n",
    "    inp = keras.layers.Input(shape=(meta[\"n_features_in_\"]))\n",
    "    model.add(inp)\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        layer = keras.layers.Dense(hidden_layer_size, activation=\"relu\")\n",
    "        model.add(layer)\n",
    "    if meta[\"target_type_\"] == \"binary\":\n",
    "        n_output_units = 1\n",
    "        output_activation = \"sigmoid\"\n",
    "    elif meta[\"target_type_\"] == \"multiclass\":\n",
    "        n_output_units = meta[\"n_classes_\"]\n",
    "        output_activation = \"softmax\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported task type: {meta['target_type_']}\")\n",
    "    out = keras.layers.Dense(n_output_units, activation=output_activation)\n",
    "    model.add(out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-underwear",
   "metadata": {},
   "source": [
    "For now, we raise a `NotImplementedError` for other target types. For an example handling multi-output target types, see the [Multi Output notebook](https://colab.research.google.com/github/adriangb/scikeras/blob/master/notebooks/MultiInput.ipynb).\n",
    "\n",
    "### 2.4 Losses and optimizer\n",
    "\n",
    "Like the output layer, the loss must match the type of classification task. Generally, it is easier and safet to allow SciKeras to compile your model for you by passing the loss to `KerasClassifier` directly (`KerasClassifier(loss=\"binary_crossentropy\")`). However, in order to implement custom logic around the choice of loss function, we compile the model ourselves within `get_clf_model`; SciKeras will not re-compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "magnetic-collection",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.926211Z",
     "iopub.status.busy": "2021-01-29T02:15:17.924997Z",
     "iopub.status.idle": "2021-01-29T02:15:17.933788Z",
     "shell.execute_reply": "2021-01-29T02:15:17.934604Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_model(hidden_layer_sizes: Iterable[int], meta: Dict[str, Any]):\n",
    "    model = keras.Sequential()\n",
    "    inp = keras.layers.Input(shape=(meta[\"n_features_in_\"]))\n",
    "    model.add(inp)\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        layer = keras.layers.Dense(hidden_layer_size, activation=\"relu\")\n",
    "        model.add(layer)\n",
    "    if meta[\"target_type_\"] == \"binary\":\n",
    "        n_output_units = 1\n",
    "        output_activation = \"sigmoid\"\n",
    "        loss = \"binary_crossentropy\"\n",
    "    elif meta[\"target_type_\"] == \"multiclass\":\n",
    "        n_output_units = meta[\"n_classes_\"]\n",
    "        output_activation = \"softmax\"\n",
    "        loss = \"sparse_categorical_crossentropy\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported task type: {meta['target_type_']}\")\n",
    "    out = keras.layers.Dense(n_output_units, activation=output_activation)\n",
    "    model.add(out)\n",
    "    model.compile(loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-fossil",
   "metadata": {},
   "source": [
    "At this point, we have a valid, compiled model. However if we want to be able to tune the optimizer, we should accept `compile_kwargs` as a parameter in `get_clf_model`. `compile_kwargs` will be a dictionary containing valid `kwargs` for `Model.compile`, so we can unpack it directly like `model.compile(**compile_kwargs)`. In this case however, we will only be taking the `optimizer` kwarg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intermediate-bloom",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.939182Z",
     "iopub.status.busy": "2021-01-29T02:15:17.937744Z",
     "iopub.status.idle": "2021-01-29T02:15:17.947187Z",
     "shell.execute_reply": "2021-01-29T02:15:17.947989Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_model(hidden_layer_sizes: Iterable[int], meta: Dict[str, Any], compile_kwargs: Dict[str, Any]):\n",
    "    model = keras.Sequential()\n",
    "    inp = keras.layers.Input(shape=(meta[\"n_features_in_\"]))\n",
    "    model.add(inp)\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        layer = keras.layers.Dense(hidden_layer_size, activation=\"relu\")\n",
    "        model.add(layer)\n",
    "    if meta[\"target_type_\"] == \"binary\":\n",
    "        n_output_units = 1\n",
    "        output_activation = \"sigmoid\"\n",
    "        loss = \"binary_crossentropy\"\n",
    "    elif meta[\"target_type_\"] == \"multiclass\":\n",
    "        n_output_units = meta[\"n_classes_\"]\n",
    "        output_activation = \"softmax\"\n",
    "        loss = \"sparse_categorical_crossentropy\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported task type: {meta['target_type_']}\")\n",
    "    out = keras.layers.Dense(n_output_units, activation=output_activation)\n",
    "    model.add(out)\n",
    "    model.compile(loss=loss, optimizer=compile_kwargs[\"optimizer\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-crime",
   "metadata": {},
   "source": [
    "### 2.5 Wrapping with SciKeras\n",
    "\n",
    "Our last step in defining our model is to wrap it with SciKeras. A couple of things to note are:\n",
    "- Every user-defined parameter in `model`/`get_clf_model` (in our case just `hidden_layer_sizes`) must be defined as a keyword argument to `KerasClassifier` with a default value.\n",
    "- Keras defaults to `\"rmsprop\"` for `optimizer`. We set it to `\"adam\"` to mimic MLPClassifier.\n",
    "- We set the learning rate for the optimizer to `0.001`, again to mimic MLPClassifier. We set this parameter using [parameter routing](https://scikeras.readthedocs.io/en/latest/advanced.html#routed-parameters).\n",
    "- Other parameters, such as `activation`, can be added similar to `hidden_layer_sizes`, but we omit them here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "simplified-nowhere",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.952461Z",
     "iopub.status.busy": "2021-01-29T02:15:17.950668Z",
     "iopub.status.idle": "2021-01-29T02:15:17.956948Z",
     "shell.execute_reply": "2021-01-29T02:15:17.957802Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = KerasClassifier(\n",
    "    model=get_clf_model,\n",
    "    hidden_layer_sizes=(100, ),\n",
    "    optimizer=\"adam\",\n",
    "    optimizer__learning_rate=0.001,\n",
    "    epochs=200,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-final",
   "metadata": {},
   "source": [
    "## 3. Testing our classifier\n",
    "\n",
    "Before continouing, we will run a small test to make sure we get somewhat reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "assured-collaboration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:17.963568Z",
     "iopub.status.busy": "2021-01-29T02:15:17.961952Z",
     "iopub.status.idle": "2021-01-29T02:15:18.004085Z",
     "shell.execute_reply": "2021-01-29T02:15:18.004894Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "discrete-shame",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:18.011210Z",
     "iopub.status.busy": "2021-01-29T02:15:18.007776Z",
     "iopub.status.idle": "2021-01-29T02:15:28.720863Z",
     "shell.execute_reply": "2021-01-29T02:15:28.721492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification()\n",
    "\n",
    "# check that fit works\n",
    "clf.fit(X, y)\n",
    "# check cross-validation score\n",
    "print(np.mean(cross_val_score(clf, X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-lesson",
   "metadata": {},
   "source": [
    "We get a score above 0.7, which is reasonable and indicates that our classifier is generally working.\n",
    "\n",
    "## 4. Self contained MLPClassifier\n",
    "\n",
    "You will have noticed that up until now, we define our Keras model in a function and pass that function to `KerasClassifier` via the `model` argument.\n",
    "\n",
    "This is convenient, but it does not give us a self-contained class that we could package within a module for users to instantiate. To do that, we need to subclass `KerasClassifier`.\n",
    "\n",
    "### 4.1 Subclassing\n",
    "\n",
    "By subclassing KerasClassifier, you can embed your Keras model into directly into your estimator class. We start by inheriting from KerasClassifier and defining an `__init__` method with all of our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "attempted-front",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:28.729708Z",
     "iopub.status.busy": "2021-01-29T02:15:28.727847Z",
     "iopub.status.idle": "2021-01-29T02:15:28.739855Z",
     "shell.execute_reply": "2021-01-29T02:15:28.739309Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(KerasClassifier):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(100, ),\n",
    "        optimizer=\"adam\",\n",
    "        optimizer__learning_rate=0.001,\n",
    "        epochs=200,\n",
    "        verbose=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-responsibility",
   "metadata": {},
   "source": [
    "Next, we will embed our model into `_keras_build_fn`, which takes the place of `get_clf_model`. Note that since this is now an part of the model, we no longer need to accept the any parameters in the function signature. We still accept `compile_kwargs` because we use it to get the optimizer initialized with all of it's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "changing-bridge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:28.748822Z",
     "iopub.status.busy": "2021-01-29T02:15:28.748149Z",
     "iopub.status.idle": "2021-01-29T02:15:28.764075Z",
     "shell.execute_reply": "2021-01-29T02:15:28.762739Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(KerasClassifier):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(100, ),\n",
    "        optimizer=\"adam\",\n",
    "        optimizer__learning_rate=0.001,\n",
    "        epochs=200,\n",
    "        verbose=0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _keras_build_fn(self, compile_kwargs: Dict[str, Any]):\n",
    "        model = keras.Sequential()\n",
    "        inp = keras.layers.Input(shape=(self.n_features_in_))\n",
    "        model.add(inp)\n",
    "        for hidden_layer_size in self.hidden_layer_sizes:\n",
    "            layer = keras.layers.Dense(hidden_layer_size, activation=\"relu\")\n",
    "            model.add(layer)\n",
    "        if self.target_type_ == \"binary\":\n",
    "            n_output_units = 1\n",
    "            output_activation = \"sigmoid\"\n",
    "            loss = \"binary_crossentropy\"\n",
    "        elif self.target_type_ == \"multiclass\":\n",
    "            n_output_units = self.n_classes_\n",
    "            output_activation = \"softmax\"\n",
    "            loss = \"sparse_categorical_crossentropy\"\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported task type: {self.target_type_}\")\n",
    "        out = keras.layers.Dense(n_output_units, activation=output_activation)\n",
    "        model.add(out)\n",
    "        model.compile(loss=loss, optimizer=compile_kwargs[\"optimizer\"])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-craps",
   "metadata": {},
   "source": [
    "Let's check that our subclassed model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "relative-mouse",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:28.769203Z",
     "iopub.status.busy": "2021-01-29T02:15:28.768557Z",
     "iopub.status.idle": "2021-01-29T02:15:37.401563Z",
     "shell.execute_reply": "2021-01-29T02:15:37.402082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9099999999999999\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier()\n",
    "\n",
    "# check cross-validation score\n",
    "print(np.mean(cross_val_score(clf, X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-divide",
   "metadata": {},
   "source": [
    "## 5. MLPRegressor\n",
    "\n",
    "The process for MLPRegressor is similar, we only change the loss function and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "curious-battlefield",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:37.411195Z",
     "iopub.status.busy": "2021-01-29T02:15:37.410557Z",
     "iopub.status.idle": "2021-01-29T02:15:37.425372Z",
     "shell.execute_reply": "2021-01-29T02:15:37.426044Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLPRegressor(KerasRegressor):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(100, ),\n",
    "        optimizer=\"adam\",\n",
    "        optimizer__learning_rate=0.001,\n",
    "        epochs=200,\n",
    "        verbose=0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _keras_build_fn(self, compile_kwargs: Dict[str, Any]):\n",
    "        model = keras.Sequential()\n",
    "        inp = keras.layers.Input(shape=(self.n_features_in_))\n",
    "        model.add(inp)\n",
    "        for hidden_layer_size in self.hidden_layer_sizes:\n",
    "            layer = keras.layers.Dense(hidden_layer_size, activation=\"relu\")\n",
    "            model.add(layer)\n",
    "        out = keras.layers.Dense(1)\n",
    "        model.add(out)\n",
    "        model.compile(loss=\"mse\", optimizer=compile_kwargs[\"optimizer\"])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "inclusive-conclusion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:37.432106Z",
     "iopub.status.busy": "2021-01-29T02:15:37.431477Z",
     "iopub.status.idle": "2021-01-29T02:15:37.435914Z",
     "shell.execute_reply": "2021-01-29T02:15:37.442216Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "interracial-training",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T02:15:37.453003Z",
     "iopub.status.busy": "2021-01-29T02:15:37.451369Z",
     "iopub.status.idle": "2021-01-29T02:15:39.330210Z",
     "shell.execute_reply": "2021-01-29T02:15:39.330700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996348650201241"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = MLPRegressor()\n",
    "\n",
    "# Define a simple linear relationship\n",
    "y = np.arange(100)\n",
    "X = (y/2).reshape(-1, 1)\n",
    "\n",
    "# check score\n",
    "reg.fit(X, y).score(X, y)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
