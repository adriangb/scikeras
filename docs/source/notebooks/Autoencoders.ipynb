{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z22BE9uhvoxO"
   },
   "source": [
    "# Autoencoders in SciKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Run in Colab](https://www.tensorflow.org/images/colab_logo_32px.png)](https://colab.research.google.com/github/adriangb/scikeras/blob/master/docs/source/notebooks/Autencoders.ipynb) Run in Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hapoJed-voxP"
   },
   "source": [
    "Autencoders are an approach to use nearual networks to distill data into it's most important features, thereby compressing the data. We will be following the [Keras tutorial](https://blog.keras.io/building-autoencoders-in-keras.html) on the topic, which goes much more in depth and breadth than we will here. You are highly encouraged to check out that tutorial if you want to learn about autoencoders in the general sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT-ibpi7voxQ"
   },
   "source": [
    "### Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekJWKPFMvoxR"
   },
   "source": [
    "* [Data](#data)\n",
    "* [Define Model](#model)\n",
    "* [Training](#training)\n",
    "* [Explore Results](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6avb3GBQDQyG"
   },
   "source": [
    "Install SciKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:02.524000Z",
     "iopub.status.busy": "2021-01-23T01:45:02.523158Z",
     "iopub.status.idle": "2021-01-23T01:45:04.168794Z",
     "shell.execute_reply": "2021-01-23T01:45:04.169192Z"
    },
    "id": "qCcyTjVkvoxR"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import scikeras\n",
    "except ImportError:\n",
    "    !python -m pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZveNcetDQyL"
   },
   "source": [
    "Silence TensorFlow warnings to keep output succint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:04.172344Z",
     "iopub.status.busy": "2021-01-23T01:45:04.171809Z",
     "iopub.status.idle": "2021-01-23T01:45:04.173474Z",
     "shell.execute_reply": "2021-01-23T01:45:04.173850Z"
    },
    "id": "ekNmO_GPDQyL"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tensorflow import get_logger\n",
    "get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCuOBH8AvoxX"
   },
   "source": [
    "<a id='data'></a>## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3fAUKBUvoxY"
   },
   "source": [
    "We load the dataset from the Keras tutorial. The dataset consists of images of cats and dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:04.178671Z",
     "iopub.status.busy": "2021-01-23T01:45:04.178131Z",
     "iopub.status.idle": "2021-01-23T01:45:04.462345Z",
     "shell.execute_reply": "2021-01-23T01:45:04.462925Z"
    },
    "id": "QM74xeoe-1S-",
    "outputId": "23946eec-c32f-4db6-8bda-e12bda6492ea"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuR10hymK0dh"
   },
   "source": [
    "<a id='model'></a>## Define Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be defining a very simple autencoder. We define _three_ model building methods:\n",
    "\n",
    "1. One to build a full end-to-end autoencoder.\n",
    "2. One to create a model that includes only the encoder portion.\n",
    "3. One that creates a model that includes only the decoder portion.\n",
    "\n",
    "The only variable we give our model is the encoding dimensions, which will be a hyperparemter of our final transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:04.471796Z",
     "iopub.status.busy": "2021-01-23T01:45:04.471053Z",
     "iopub.status.idle": "2021-01-23T01:45:04.472983Z",
     "shell.execute_reply": "2021-01-23T01:45:04.473568Z"
    },
    "id": "dBFFXT-__7KU"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def get_fit_model(encoding_dim: int) -> keras.Model:\n",
    "    \"\"\"Get an autoencoder.\n",
    "\n",
    "    This autoencoder compresses a 28x28 image (784 pixels) down to a feature of length\n",
    "    `encoding_dim`, and tries to reconstruct the input image from that vector.\n",
    "    \"\"\"\n",
    "    input_img = keras.Input(shape=(784,), name=\"input\")\n",
    "    encoded = keras.layers.Dense(encoding_dim, activation='relu', name=\"encoded\")(input_img)\n",
    "    decoded = keras.layers.Dense(784, activation='sigmoid', name=\"output\")(encoded)\n",
    "    autoencoder_model = keras.Model(input_img, decoded)\n",
    "    return autoencoder_model\n",
    "\n",
    "def get_tf_model(fit_model: keras.Model) -> keras.Model:\n",
    "    \"\"\"Get an encoder model.\n",
    "\n",
    "    We do this by extracting the encoding layer from the fitted autoencoder model.\n",
    "    \"\"\"\n",
    "    return keras.Model(fit_model.get_layer(\"input\").input, fit_model.get_layer(\"encoded\").output)\n",
    "\n",
    "def get_inverse_tf_model(fit_model: keras.Model, encoding_dim: int) -> keras.Model:\n",
    "    \"\"\"Get an deencoder model.\n",
    "\n",
    "    We do this by extracting the deencoding layer from the fitted autoencoder model\n",
    "    and adding a new Keras input layer.\n",
    "    \"\"\"\n",
    "    encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "    output = fit_model.get_layer(\"output\")(encoded_input)\n",
    "    return keras.Model(encoded_input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a class that that will enable the `transform` and `fit_transform` methods, as well as integrating all three of our models into a single estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:04.483566Z",
     "iopub.status.busy": "2021-01-23T01:45:04.482609Z",
     "iopub.status.idle": "2021-01-23T01:45:04.876484Z",
     "shell.execute_reply": "2021-01-23T01:45:04.876891Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, clone\n",
    "from scikeras.wrappers import BaseWrapper\n",
    "\n",
    "\n",
    "class KerasTransformer(BaseWrapper, TransformerMixin):\n",
    "    \"\"\"A class that enables transform and fit_transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, tf_est: BaseWrapper = None, inv_tf_est: BaseWrapper = None, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tf_est = tf_est\n",
    "        self.inv_tf_est = inv_tf_est\n",
    "\n",
    "\n",
    "    def fit(self, X, sample_weight=None):\n",
    "        super().fit(X=X, y=X, sample_weight=sample_weight)\n",
    "        self.tf_est_ = clone(self.tf_est)\n",
    "        self.inv_tf_est_ = clone(self.inv_tf_est)\n",
    "        self.tf_est_.set_params(fit_model=self.model_)\n",
    "        self.inv_tf_est_.set_params(fit_model=self.model_, encoding_dim=self.encoding_dim)\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        self.tf_est_.initialize(X=X)\n",
    "        X_tf = self.tf_est_.predict(X=X)\n",
    "        self.inv_tf_est_.initialize(X_tf)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        X_tf = self.tf_est_.predict(X)\n",
    "        return X_tf\n",
    "    \n",
    "    def inverse_transform(self, X_tf):\n",
    "        X = self.inv_tf_est_.predict(X_tf)\n",
    "        X = self.feature_encoder_.inverse_transform(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we wrap the Keras Model with Scikeras. Note that for our encoder/decoder estimators, we do not need to provide a loss function since no training will be done. We do however need to have the `fit_model` and `encoding_dim` so that these will be settable by `BaseWrapper.set_params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:04.881324Z",
     "iopub.status.busy": "2021-01-23T01:45:04.880468Z",
     "iopub.status.idle": "2021-01-23T01:45:04.882563Z",
     "shell.execute_reply": "2021-01-23T01:45:04.882956Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_est = BaseWrapper(model=get_tf_model, fit_model=None, verbose=0)\n",
    "inv_tf_est = BaseWrapper(model=get_inverse_tf_model, fit_model=None, encoding_dim=None, verbose=0)\n",
    "autoencoder = KerasTransformer(model=get_fit_model, tf_est=tf_est, inv_tf_est=inv_tf_est, loss=\"binary_crossentropy\", encoding_dim=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqBl5xLDP43O"
   },
   "source": [
    "<a id='training'></a>## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fHbt_mUVBhE"
   },
   "source": [
    "To train the model, we pass the input images as both the features and the target. This will train the layers to compress the data as accurately as possible between the encoder and decoder. Note that we only pass the `X` parameter, since we defined the mapping `y=X` in `KerasTransformer.fit` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:04.886009Z",
     "iopub.status.busy": "2021-01-23T01:45:04.885471Z",
     "iopub.status.idle": "2021-01-23T01:45:15.293619Z",
     "shell.execute_reply": "2021-01-23T01:45:15.294126Z"
    },
    "id": "QQhtIWsMP4Ii",
    "outputId": "1581384e-7012-4816-d4cf-a4190a30e50f"
   },
   "outputs": [],
   "source": [
    "_ = autoencoder.fit(X=x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we round trip the test dataset and explore the performance of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:15.299168Z",
     "iopub.status.busy": "2021-01-23T01:45:15.298396Z",
     "iopub.status.idle": "2021-01-23T01:45:15.652515Z",
     "shell.execute_reply": "2021-01-23T01:45:15.653112Z"
    }
   },
   "outputs": [],
   "source": [
    "roundtrip_imgs = autoencoder.inverse_transform(autoencoder.transform(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='results'></a>## Explore Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our inputs to lossy decoded outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:15.660745Z",
     "iopub.status.busy": "2021-01-23T01:45:15.659832Z",
     "iopub.status.idle": "2021-01-23T01:45:16.566747Z",
     "shell.execute_reply": "2021-01-23T01:45:16.567137Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(roundtrip_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the compression? Let's check the sizes of the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T01:45:16.570923Z",
     "iopub.status.busy": "2021-01-23T01:45:16.570287Z",
     "iopub.status.idle": "2021-01-23T01:45:16.750731Z",
     "shell.execute_reply": "2021-01-23T01:45:16.751121Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.transform(x_test)\n",
    "print(f\"x_test.shape[1]: {x_test.shape[1]}\")\n",
    "print(f\"encoded_imgs.shape[1]: {encoded_imgs.shape[1]}\")\n",
    "cr = round((encoded_imgs.nbytes/x_test.nbytes), 2)\n",
    "print(f\"Compression ratio: 1/{1/cr:.0f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Benchmarks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
